{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c27444e",
   "metadata": {},
   "source": [
    "# Some interesting background theory\n",
    "\n",
    "## This document outlines the theoretical foundations used the framework, including metric definitions/interpretations statistical methods and algorithms\n",
    "\n",
    "---\n",
    "\n",
    "# Basics\n",
    "### Sharpe Ratio\n",
    "$$\n",
    "\\text{Sharpe} = \\frac{(\\mathbb{E}[r] - r_f)}{\\sigma} \\cdot \\sqrt{T}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbb{E}[r]$: expected return  \n",
    "- $\\sigma$: standard deviation  \n",
    "- $T$: annualization factor (using 365 days as base for crypto)\n",
    "- $r_f$: risk-free return rate which usually derived from treasury bills. for crypto its typically 0 \n",
    "\n",
    "### Sortino Ratio\n",
    "\n",
    "$$\n",
    "\\text{Sortino} = \\frac{(\\mathbb{E}[r] - r_f)}{\\sigma_{\\text{down}}} \\cdot \\sqrt{T}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\sigma_{\\text{down}}$: downside standard deviation (only measures negative return deviations)\n",
    "\n",
    "Unlike the Sharpe Ratio, the Sortino Ratio **only penalizes downside volatility**, which is a more realistic risk measure, especially for strategies with asymmetric return distributions.\n",
    "\n",
    "### Calmar Ratio\n",
    "\n",
    "$$\n",
    "\\text{Calmar} = \\frac{\\text{CAGR}}{|\\text{Max Drawdown}|}, \\quad \\text{where} \\quad \n",
    "\\text{CAGR} = \\left(1 + \\text{Cumulative Return} \\right)^{T / N} - 1\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N$: total number of periods in the return series\n",
    "- Cumulative Return = $\\prod (1 + r_t) - 1$\n",
    "\n",
    "---\n",
    "\n",
    "# Montecarlo Simulation\n",
    "## Why?\n",
    "Using Monte Carlo methods gives you a better understanding of real risks in your trading system. In general we rely here on the **Weak Law of Large Numbers (WLLN)** and the **Central Limit Theorem (CLT)** from probability theory:\n",
    "\n",
    "### Weak Law of Large Numbers (WLLN)\n",
    "$$\n",
    "\\lim_{N \\to \\infty} \\Pr\\left(|\\bar X_N - \\mathbb{E}[X]| > \\varepsilon\\right) = 0,\\quad \\text{where} \\quad \\bar X_N = \\frac{1}{N} \\sum_{i=1}^N X_i,\\; \\varepsilon > 0\n",
    "$$\n",
    "\n",
    "### Central Limit Theorem (CLT)\n",
    "$$\n",
    "\\frac{\\bar X_N - \\mathbb{E}[X]}{\\sigma / \\sqrt{N}} \\xrightarrow{d} \\mathcal{N}(0,1),\\quad \\text{with} \\quad \\sigma^2 = \\mathrm{Var}(X)\n",
    "$$\n",
    "\n",
    "In this framework, we apply for example simple bootstrap resampling with replacement to generate thousands of synthetic equity curves. While this breaks temporal dependencies such as autocorrelation and volatility clustering, it provides a first-order approximation of the sampling distribution of key performance metrics (Sharpe, Sortino, Calmar, ...).\n",
    "\n",
    "The statistical rationale rests on two things:\n",
    "1. Weak Law of Large Numbers: With enough resamples, the bootstrapped estimates stabilise around their expected values\n",
    "2. Asymptotic normality (heuristically linked to the CLT): The empirical distributions tend to become approximately normal allowing us to derive confidence intervals and p-values\n",
    "\n",
    "Although the strict i.i.d. assumptions are violated, empirical evidence often shows sufficiently normal-shaped distributions. If strong serial dependence is suspected, a block or stationary bootstrap is preferable.\n",
    "\n",
    "---\n",
    "\n",
    "# Walkfoward Optimization and Generalization loss\n",
    "## Why ?\n",
    "\n",
    "Relying on a single train-test split where you optimize your hyperparameters on in-sample data (train fold) and evaluate on out-of-sample data (test fold) is better than nothing, but still prone to overfitting.\n",
    "\n",
    "A more robust approach is **Walkforward Optimization**, where you use a rolling (or anchored) train/test window. This generates multiple smaller train-test splits across the entire dataset, providing a more reliable estimate of generalization performance and robustness.\n",
    "\n",
    "One step further is using a **Generalization loss function** which comes from machine-learning training and acts as penalty for massive underperfromance on unseend data and prevents even more overfitting. There are many different ways to define your GL-function. One for example:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\overline{\\text{ValMetric}} + \\beta \\cdot \\frac{\\max(\\text{GL})}{\\text{scale}}\n",
    "$$\n",
    "\n",
    "> **Note:** This formula would penalizes even when OOS > OOS_Benchmark < IS.\n",
    "\n",
    "This penalizes sharp degradation between in-sample and out-of-sample performance, especially when itâ€™s unstable over recent evaluations.\n",
    "\n",
    "---\n",
    "\n",
    "# Tail Risk & Dependeces\n",
    "## Why ?\n",
    "\n",
    "Extreme losses rarely happen in isolation during market stress, correlations surge and multiple assets drop together. Tail risk captures these rare but severe events that standard metrics like variance or VaR often miss.\n",
    "\n",
    "To measure and manage this risk realistically, we need models that account for joint tail behavior. This is essential for understanding systemic risk and stress scenarios.\n",
    "\n",
    "### CVaR\n",
    "\n",
    "\n",
    "\n",
    "### Copulas"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
